# Topic Modeling - Gu√≠a de Uso

Este directorio contiene el pipeline de modelado de t√≥picos usando BERTopic.

## üìÅ Archivos

- `model.py`: Clase principal `TopicModelingPipeline` con todo el pipeline
- `predict.py`: Scripts de ejemplo para hacer predicciones con modelos guardados

## üöÄ Uso

### 1. Entrenar un modelo nuevo

```python
from src.model.model import TopicModelingPipeline
from src.settings.settings import DATA_PATH, IMAGES_DIR

# Crear y ejecutar el pipeline
pipeline = TopicModelingPipeline(data_path=DATA_PATH, images_dir=IMAGES_DIR)
pipeline.run_pipeline()

# El modelo se guardar√° autom√°ticamente en ./model/topic_model.pkl
```

O desde la terminal:

```bash
python -m src.model.model
```

### 2. Cargar un modelo guardado

```python
from src.model.model import TopicModelingPipeline

# Opci√≥n 1: Usar el m√©todo est√°tico
topic_model = TopicModelingPipeline.load_topic_model("./model/topic_model.pkl")

# Opci√≥n 2: Cargar en un pipeline existente
pipeline = TopicModelingPipeline(data_path="", images_dir="images")
pipeline.topic_model = TopicModelingPipeline.load_topic_model("./model/topic_model.pkl")
```

### 3. Hacer predicciones con nuevos documentos

#### M√∫ltiples documentos:

```python
from src.model.model import TopicModelingPipeline

# Inicializar y cargar modelo
pipeline = TopicModelingPipeline(data_path="", images_dir="images")
pipeline.topic_model = TopicModelingPipeline.load_topic_model("./model/topic_model.pkl")

# Nuevos documentos
documentos = [
    "Esta aplicaci√≥n es excelente",
    "La app se crashea mucho",
    "El delivery es muy r√°pido"
]

# Predecir t√≥picos
topics, probabilities = pipeline.predict_topics(documentos)

# Resultados
for doc, topic, prob in zip(documentos, topics, probabilities):
    print(f"Documento: {doc}")
    print(f"T√≥pico: {topic}, Probabilidad: {prob:.4f}\n")
```

#### Un solo documento:

```python
from src.model.model import TopicModelingPipeline

pipeline = TopicModelingPipeline(data_path="", images_dir="images")
pipeline.topic_model = TopicModelingPipeline.load_topic_model("./model/topic_model.pkl")

documento = "La calidad de las entregas es excelente"
topics, probs = pipeline.predict_topics(documento)

print(f"T√≥pico asignado: {topics[0]}")
print(f"Probabilidad: {probs[0]:.4f}")
```

### 4. Obtener informaci√≥n de los t√≥picos

#### Ver todos los t√≥picos:

```python
# Obtener informaci√≥n de todos los t√≥picos
topic_info = pipeline.get_topic_info()
print(topic_info.head(10))
```

#### Ver un t√≥pico espec√≠fico:

```python
# Obtener palabras clave de un t√≥pico espec√≠fico
topic_id = 0
topic_words = pipeline.get_topic_info(topic_id=topic_id)
print(f"\nPalabras del t√≥pico {topic_id}:")
print(topic_words)
```

### 5. Usar el script de predicci√≥n de ejemplo

```bash
# Ejecutar ejemplos de predicci√≥n
python -m src.model.predict
```

## üîß M√©todos principales

### `TopicModelingPipeline`

#### Entrenamiento:

- `load_data()`: Carga datos desde CSV
- `train_embedding_model()`: Inicializa el modelo de embeddings
- `create_embeddings()`: Crea embeddings para los documentos
- `reduce_dimensionality(n_components)`: Reduce dimensionalidad con UMAP
- `cluster_documents(reduced_embeddings)`: Agrupa documentos con HDBSCAN
- `train_topic_model()`: Entrena el modelo BERTopic
- `visualize_topic_model()`: Genera visualizaciones
- `run_pipeline()`: Ejecuta todo el pipeline completo

#### Inferencia:

- `save_topic_model(path)`: Guarda el modelo entrenado
- `load_topic_model(path)`: [Est√°tico] Carga un modelo guardado
- `predict_topics(documents)`: Predice t√≥picos para nuevos documentos
- `get_topic_info(topic_id)`: Obtiene informaci√≥n de t√≥picos

## üìä Visualizaciones generadas

El pipeline genera autom√°ticamente las siguientes visualizaciones en `images/`:

- `topics.png`: Visualizaci√≥n 2D de t√≥picos (requiere ‚â•3 t√≥picos)
- `barchart.png`: Gr√°fico de barras con los t√≥picos m√°s frecuentes
- `heatmap.png`: Mapa de calor de similitud entre t√≥picos
- `hierarchy.png`: Jerarqu√≠a de t√≥picos
- `clusters.png`: Visualizaci√≥n de clusters de documentos

## ‚öôÔ∏è Configuraci√≥n

Los par√°metros del modelo se configuran en `src/settings/settings.py`:

```python
EMBEDDING_MODEL = "Alibaba-NLP/gte-Qwen2-1.5B-instruct"
UMAP_COMPONENTS_CLUSTER = 5
UMAP_COMPONENTS_VIZ = 2
HDBSCAN_MIN_CLUSTER_SIZE = 5  # Ajustar seg√∫n tama√±o del dataset
RANDOM_STATE = 42
```

### Recomendaciones para `HDBSCAN_MIN_CLUSTER_SIZE`:

- Dataset peque√±o (< 1,000 documentos): 5-15
- Dataset mediano (1,000-10,000): 15-50
- Dataset grande (> 10,000): 50-100

## üéØ Ejemplos de uso completo

Ver `predict.py` para ejemplos completos de:

1. Predicci√≥n de m√∫ltiples documentos
2. Predicci√≥n de un solo documento
3. An√°lisis de t√≥picos asignados
